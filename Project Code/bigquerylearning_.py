# -*- coding: utf-8 -*-
"""BigQueryLearning_Zoe_Pacalin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11DN7pymMBT9OBjIbuI_jchrMIpYaG1t6

# Linear and Logistic Regression
---

## Setting Up BigQuery and Dependencies
"""

# Run this cell to authenticate yourself to BigQuery
from google.colab import auth
auth.authenticate_user()
project_id = "imposing-coast-218404"

# Initialize BiqQuery client
from google.cloud import bigquery
client = bigquery.Client(project="imposing-coast-218404")  # pass in your projectid

import altair as alt
import pandas as pd

"""Link to our data: https://collegescorecard.ed.gov/data/

This work was done by Zoe Pacalin for both CS145 and CS229. 145 prompted in depth data exploration that served as a launching point for my 229 final project. The work for 145 was done with the help of Claire Rosenfeld.

####1. Admission Rate

First, we build 4 dataframes which we then concatenate so that we can create multiple tuples from a single tuple in the original dataset. Namely, we wish to capture admit rate to earnings and add the attribute "category", where catergory classifies "earnings" based on it's original attribute label (mean_10, mean_6, med_10, med_6).

%%bigquery --project $project_id admit_rate_mean10

SELECT ROUND(cast(ADM_RATE as FLOAT64),2) as admit_rate, AVG(cast(MN_EARN_WNE_P10 as FLOAT64)) as Statistic
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND
  ADM_RATE IS NOT NULL 
GROUP BY admit_rate

%%bigquery --project $project_id admit_rate_mean6

SELECT ROUND(cast(ADM_RATE as FLOAT64),2) as admit_rate, AVG(cast(MN_EARN_WNE_P6 as FLOAT64)) as Statistic
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE MN_EARN_WNE_P6 != 'None' AND 
  MN_EARN_WNE_P6 != 'PrivacySuppressed' AND
  ADM_RATE IS NOT NULL 
GROUP BY admit_rate

%%bigquery --project $project_id admit_rate_med10

SELECT ROUND(cast(ADM_RATE as FLOAT64),2) as admit_rate, AVG(cast(MD_EARN_WNE_P10 as FLOAT64)) as Statistic
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE MD_EARN_WNE_P10 != 'None' AND 
  MD_EARN_WNE_P10 != 'PrivacySuppressed' AND
  ADM_RATE IS NOT NULL 
GROUP BY admit_rate

%%bigquery --project $project_id admit_rate_med6

SELECT ROUND(cast(ADM_RATE as FLOAT64),2) as admit_rate, AVG(cast(MD_EARN_WNE_P6 as FLOAT64)) as Statistic
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE MD_EARN_WNE_P6 != 'None' AND 
  MD_EARN_WNE_P6 != 'PrivacySuppressed' AND
  ADM_RATE IS NOT NULL 
GROUP BY admit_rate
"""

admit_rate_mean10 = admit_rate_mean10.assign(category='Mean_10')
admit_rate_mean6 = admit_rate_mean6.assign(category='Mean_6')
admit_rate_med10 = admit_rate_med10.assign(category='Med_10')
admit_rate_med6 = admit_rate_med6.assign(category='Med_6')

frames = [admit_rate_mean10, admit_rate_mean6, admit_rate_med10, admit_rate_med6]

consolidated = pd.concat(frames)
cor = consolidated['admit_rate'].corr(consolidated['Statistic'])
print('Correlation between admission rate and earnings is moderate:', -cor)

alt.Chart(consolidated).mark_point().encode(
    x = alt.X('admit_rate', sort = 'descending'),
    y = alt.Y('Statistic'),
    color = 'category'
).properties(
    width=800, height=400, title = 'Earnings to Admit Rate'
)

"""Earnings are much more precisely predicable by admit rate when admit rate is greater than 40%. Beyond this threshold, there is much greater variability, and earnings for extremely selective institutions begins to grow exponentially. The future earnings are skew right in these selective instutions as is reflected by the higher means than medians, and this skewness grows over time (greater for 10 years post entry than for 10). These observations suggest that only for students from selective institutions do earnings grow at an increasing rate after completion.




####2. Test Scores

Next, we consider selectivity by the metric of test scores. We perform similar data manipulations in order to visulatize the relationship bewteen SAT/ACT scores and earnings, but to simplify/reduce number of queries, we will use mean earnings 10 years after entry as the metric, since that showed the greatest variability in our previous analysis based on admission rate.

%%bigquery --project $project_id sat

SELECT cast(SAT_AVG as FLOAT64) as Avg_SAT_cum, ROUND(AVG(cast(MN_EARN_WNE_P10 as FLOAT64))) as Earnings
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE SAT_AVG is not NULL AND MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND SAT_AVG is not NULL
GROUP BY Avg_SAT_cum
ORDER BY Avg_SAT_cum

%%bigquery --project $project_id act

SELECT cast(ACTCMMID as FLOAT64) as Midpoint_ACT_cum, ROUND(AVG(cast(MN_EARN_WNE_P10 as FLOAT64))) as Earnings
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE ACTCMMID is not NULL AND MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND ACTCMMID is not NULL
GROUP BY Midpoint_ACT_cum
ORDER BY Midpoint_ACT_cum
"""

cor_sat = sat['Avg_SAT_cum'].corr(sat['Earnings'])
print('Correlation between SAT average and earnings (mean_10) is:', cor_sat)
cor_act = act['Midpoint_ACT_cum'].corr(act['Earnings'])
print('Correlation between ACT median and earnings (mean_10) is:', cor_act)

perc_dif = 100*(cor_sat-cor_act)/2
rel_dif = 100*(cor_sat-cor_act)/((cor_sat+cor_act)/2)
print('This difference is', perc_dif, 'percent different over the range of r and',rel_dif,'percent different relative to their average')

"""%%bigquery --project $project_id sat_read

SELECT cast(SATVRMID as FLOAT64) as Score, ROUND(AVG(cast(MN_EARN_WNE_P10 as FLOAT64))) as Earnings
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE SATVRMID is not NULL AND MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND SATVRMID is not NULL
GROUP BY Score
ORDER BY Score

%%bigquery --project $project_id sat_math

SELECT cast(SATMTMID as FLOAT64) as Score, ROUND(AVG(cast(MN_EARN_WNE_P10 as FLOAT64))) as Earnings
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE SATMTMID is not NULL AND MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND SATMTMID is not NULL
GROUP BY Score
ORDER BY Score

%%bigquery --project $project_id sat_write

SELECT cast(SATWRMID as FLOAT64) as Score, ROUND(AVG(cast(MN_EARN_WNE_P10 as FLOAT64))) as Earnings
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE SATWRMID is not NULL AND MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND SATWRMID is not NULL
GROUP BY Score
ORDER BY Score
"""

sat_read = sat_read.assign(category='Reading')
sat_math = sat_math.assign(category='Math')
sat_write = sat_write.assign(category='Writing')

frames = [sat_read, sat_math, sat_write]

consolidated = pd.concat(frames)
cor_read = sat_read['Score'].corr(consolidated['Earnings'])
cor_write = sat_write['Score'].corr(consolidated['Earnings'])
cor_math = sat_math['Score'].corr(consolidated['Earnings'])
print('Correlation between earnings is scores for reading, writing and math are, respectively,', cor_read, cor_write, cor_math)
print('The SAT writing seems the least indicative of future success. The ACT subject score correlations may differ.')

chart1 = alt.Chart(consolidated).mark_point().encode(
    x = alt.X('Score'),
    y = alt.Y('Earnings'),
    color = 'category'
).properties(
    width=400, height=400, title = 'Earnings to SAT Scores'
)

chart2 = alt.Chart(sat).mark_point().encode(
    x = alt.X('Avg_SAT_cum'),
    y = alt.Y('Earnings')
).properties(
    width=400, height=400, title = 'Earnings to SAT Average'
)



chart2 | chart1

"""We see from the correlation computations and from the visualizations that the distribution of average to subject scoes is similar; thus, moving forward we may be able to safely use the average SAT score as the single metric (rather than carrying all 3). In fact, the average SAT score has a higher correlation coefficient with future earnings than any individual subject AND the average of the correlations of the individual subjects. The SAT average is slightly more correlated with earnings than the ACT, and we will therefore consider it in future feature selection (rather than ACT).

______

###B. Financial Investment in Education

####1. Tuition

The following attributes quantify the average tuition price. First we consider them separately (public or private) and then we disregard public/private and consider the cost simply under a single variable: "tuition". We use the mean earnings 10 years after entry as a single metric for earnings.

    

|  code  |             description          
|-------------|---------------------------      
|    NPT4_PUB    |        Average net price for Title IV institutions (public institutions)    
|    NPT4_PRIV    |       Average net price for Title IV institutions (private for-profit and nonprofit institutions)

%%bigquery --project $project_id cost

SELECT NPT4_PUB, NPT4_PRIV, ROUND(cast(MN_EARN_WNE_P10 as FLOAT64)) as Earnings
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND
  (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)
"""

cor_pub = cost['NPT4_PUB'].corr(cost['Earnings'])
pub_avg = cost['NPT4_PUB'].mean()
pub_Eavg = cost[cost['NPT4_PUB']>0].mean()["Earnings"]
pub_med = cost['NPT4_PUB'].median()
pub_std = cost['NPT4_PUB'].std()

cor_priv = cost['NPT4_PRIV'].corr(cost['Earnings'])
priv_avg = cost['NPT4_PRIV'].mean()
priv_Eavg = cost[cost['NPT4_PRIV']>0].mean()["Earnings"]
priv_med = cost['NPT4_PRIV'].median()
priv_std = cost['NPT4_PRIV'].std()

print('Public Institutions: ')
print('Correlation, tuition to earnings: ', cor_pub)
print('Average cost: ', round(pub_avg))
print('Median cost: ', pub_med)
print('Average earnings: ',pub_Eavg)
print()
print('Private Institutions: ')
print('Correlation, tuition to earnings: ', cor_priv)
print('Average cost: ', round(priv_avg,0))
print('Median cost: ', priv_med)
print('Average earnings: ',priv_Eavg)

data = cost.sample(1000)

chart1 = alt.Chart(data).mark_point().encode(
    x = alt.X('NPT4_PUB'),
    y = alt.Y('Earnings')
).properties(
    width=400, height=400, title = 'Earnings to Tuition Public'
)

chart2 = alt.Chart(data).mark_point().encode(
    x = alt.X('NPT4_PRIV'),
    y = alt.Y('Earnings')
).properties(
    width=400, height=400, title = 'Earnings to Tuition Private'
)
chart1 | chart2

"""With no significant difference in earnings on average 10 years after entry, if anything a slightly lower amount, private institutions charge more than double for a more weakly-correlated earnings prediction! Because of the dramatic difference in average tuition, a much lower tuition if at a public institution predicts a higher income (10 years out) than the same would for private. For example, 10K at a public ranges from 30-40K  while at a private school, 10K tuition centers around 25K earnings. Therefore tuition must be qualified by tuition type when used as an input feature in a machine learning model. 

One strategy would be to adjust tuition/normalize one to the other (what might 20K private tuition correspond to for a public school tuition). We could compute this by considering the number of standard deviations from the mean of a private institution a private tuition is, and find that z score on the distribution of public tuitions.

####2. Student Finances: Debt and Aid (Pell Grant)

PCTPELL : description: Percentage of undergraduates who receive a Pell Grant

%%bigquery --project $project_id pell_to_earnings10 

SELECT round(cast(PCTPELL as float64),2) as Pell_percentage, AVG(cast(MN_EARN_WNE_P10 as int64)) as AVG_EARN_10YRS
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' 
GROUP BY Pell_percentage
"""

cor = pell_to_earnings10['Pell_percentage'].corr(pell_to_earnings10['AVG_EARN_10YRS'])
print('Correlation between pell recipient % and earnings 10 years later is',cor)

alt.Chart(pell_to_earnings10).mark_point().encode(
    x= alt.X('Pell_percentage'),
    y=alt.Y('AVG_EARN_10YRS')
).properties(
    title='Pell Recipienct Percentage to mean earnings of students working & not enrolled 10 years after entry',
    width=900, height=500
)

"""This reflects that institutions with fewer very low socio-economic standing students produce average earnings that are 3x + greater than schools with high concentrations of Pell recipients.

%%bigquery --project $project_id debt_to_earnings10 

SELECT cast(DEBT_MDN as float64) as DEBT, AVG(cast(MN_EARN_WNE_P10 as int64)) as AVG_EARN_10YRS
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND
  DEBT_MDN != 'None'AND
  DEBT_MDN != 'PrivacySuppressed'
GROUP BY DEBT
"""

cor = debt_to_earnings10['DEBT'].corr(debt_to_earnings10['AVG_EARN_10YRS'])
print('Correlation from all data (not the sample of 1000 used to plot) between DEBT median and earnings 10 years later is',cor)

data = debt_to_earnings10.sample(1000)

alt.Chart(data).mark_point().encode(
    x= alt.X('DEBT'),
    y=alt.Y('AVG_EARN_10YRS')#, scale = alt.Scale(type='log',base=10))
).properties(
    title='median original amount of loan upon entering repayment to mean earnings of students working & not enrolled 10 years after entry',
    width=900, height=500
)

"""The graph above depicts the relationship between median original amount of the loan principal upon entering repayment and mean earnings of students working and not enrolled 10 years after entry. 
Note that PrivacySuppressed and None values are excluded, and we hope/assume that data have such values at random, and that these exceptions do.

Surprisingly, greater debt is moderately correlated with greater earnings -- how uplifting!

####3. Institutional Financial Investment



|  code  |             description          
|-------------|---------------------------      
|    INEXPFTE    |        Instructional expenditures per full-time equivalent student    
|    AVGFACSAL    |       Average faculty salary 
|    PFTFAC    |       Proportion faculty full-time

%%bigquery --project $project_id inst_financial 

SELECT cast(INEXPFTE as float64) as Spent_Teach_Per_Student, AVG(cast(MN_EARN_WNE_P10 as int64)) as AVG_EARN_10YRS
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' AND INEXPFTE is not NULL
GROUP BY Spent_Teach_Per_Student
ORDER BY Spent_Teach_Per_Student
"""

cor = inst_financial['Spent_Teach_Per_Student'].corr(inst_financial['AVG_EARN_10YRS'])
print('Correlation from all data (not the sample of 1000 used to plot) between Spent_Teach_Per_Student and earnings 10 years later is',cor)

data = inst_financial.sample(800)

alt.Chart(data).mark_point().encode(
    x= alt.X('Spent_Teach_Per_Student'),
    y=alt.Y('AVG_EARN_10YRS')
).properties(
    title='Spending Per Student to Earnings',
    width=900, height=500
)

"""Clustering and outliers. These may wanted to be treated differentially.

%%bigquery --project $project_id inst_fac_salary

SELECT cast(AVGFACSAL as float64) as Avg_Faculty_Salary, AVG(cast(MN_EARN_WNE_P10 as int64)) as AVG_EARN_10YRS
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10 != 'None' AND 
  MN_EARN_WNE_P10 != 'PrivacySuppressed' 
GROUP BY Avg_Faculty_Salary
"""

cor = inst_fac_salary['Avg_Faculty_Salary'].corr(inst_fac_salary['AVG_EARN_10YRS'])
print('Correlation from all data (not the sample of 1000 used to plot) between Avg_Faculty_Salary and earnings 10 years later is',cor)

data = inst_fac_salary.sample(1000)

alt.Chart(data).mark_point().encode(
    x= alt.X('Avg_Faculty_Salary'),
    y=alt.Y('AVG_EARN_10YRS')
).properties(
    title='Avg_Faculty_Salary to mean earnings of students working & Earning 10 years after entry',
    width=900, height=500
)

"""##Predictions

Our plan is to model the mean earnings of students 10 years after they entered college. We are curious to explore what features can predict this metric. We began by creating a linear regression model with tuition.
"""

#1st training: with tuition
# %%bigquery --project $project_id

CREATE OR REPLACE MODEL `CollegeScorecard.earnings_model`
OPTIONS(model_type='linear_reg') AS

SELECT 
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)
  AND MOD(int64_field_0,3)=0

#1st stats
# %%bigquery --project $project_id

SELECT *
FROM ML.TRAINING_INFO(MODEL CollegeScorecard.earnings_model)

#1st evaluation
# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)
  AND MOD(int64_field_0,3)=1))

"""We weren't surprised that tuition is a big factor in predicting future earnings. More expensive schools tend to result in higher incomes. 

Next, we decided to retrain the model with a school's admissions rate, since we had previously observed that that also correlates to earnings.
"""

#2nd training: with admissions rate, tuition
# %%bigquery --project $project_id

CREATE OR REPLACE MODEL `CollegeScorecard.earnings_model`
OPTIONS(model_type='linear_reg') AS

SELECT 
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND ADM_RATE is not NULL 
  AND MOD(int64_field_0,3)=0

#2nd evaluation: admissions rate, tuition
# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND ADM_RATE is not NULL 
  AND MOD(int64_field_0,3)=1))

"""The error only decreased by ~$400 and we felt like the model could use more features. For this third round of training, we added a school's average SAT score."""

#3rd training: with average SAT, admissions rate, tuition
# %%bigquery --project $project_id

CREATE OR REPLACE MODEL `CollegeScorecard.earnings_model`
OPTIONS(model_type='linear_reg') AS

SELECT 
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND ADM_RATE is not NULL 
  AND SAT_AVG is not NULL
  AND MOD(int64_field_0,3)=0

#3rd evaluation: average SAT, admissions rate, tuition
# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND ADM_RATE is not NULL 
  AND SAT_AVG is not NULL
  AND MOD(int64_field_0,3)=1))

"""Adding the SAT score to the prediction dramatically reduced the error. Next, we were curious to see how incorporating the amount of money spent per student would affect our predictions. Our exploration earlier had indicated that the more money schools spend on students, the higher the eventual earnings."""

#4th training: with per student spending, average SAT, admissions rate, tuition
# %%bigquery --project $project_id

CREATE OR REPLACE MODEL `CollegeScorecard.earnings_model`
OPTIONS(model_type='linear_reg') AS

SELECT 
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND ADM_RATE is not NULL 
  AND SAT_AVG is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=0

#4th evaluation: with per student spending, average SAT, admissions rate, tuition

# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND SAT_AVG is not NULL 
  AND ADM_RATE is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=1))

#Evaluate final model on test set

# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND SAT_AVG is not NULL 
  AND ADM_RATE is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=2))

#Test set predictions juxtaposed with "real" values
# %%bigquery --project $project_id to_graph


SELECT
  INSTNM as institution, 
  ABS((cast(predicted_label as INT64))-real_earnings) as error,
  ABS((cast(predicted_label as INT64))-real_earnings)*100/real_earnings as perc_error,
  cast(predicted_label as INT64) as predicted_earnings, real_earnings,
  feature1 as tuition, feature2 as adm_rate, feature3 as avg_sat, 
  feature4 as per_student_spending 
FROM
  ML.PREDICT(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4,
  cast(MN_EARN_WNE_P10 as INT64) as real_earnings,
  INSTNM
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND SAT_AVG is not NULL 
  AND ADM_RATE is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=2))
ORDER BY perc_error

import altair as alt

alt.Chart(to_graph).mark_point().encode(
  x = 'predicted_earnings',
  y = 'real_earnings',
  color = 'real_earnings',
  tooltip=['predicted_earnings', 'real_earnings', 'tuition', 'adm_rate','avg_sat','per_student_spending','institution']
).properties(title="Accuracy of linear regression predictions")

import altair as alt

chart1 = alt.Chart(to_graph).mark_point().encode(
  x = 'tuition',
  y = 'error',
   color = 'real_earnings',
  tooltip=['predicted_earnings', 'real_earnings', 'tuition', 'adm_rate','avg_sat','per_student_spending','institution']
).properties(title="Tuition to prediction error")


chart2 = alt.Chart(to_graph).mark_point().encode(
  x = 'adm_rate',
  y = 'error',
      color = 'real_earnings',
  tooltip=['predicted_earnings', 'real_earnings', 'tuition', 'adm_rate','avg_sat','per_student_spending','institution']
).properties(title="Admissions rate to prediction error")


chart3 = alt.Chart(to_graph).mark_point().encode(
  x = 'avg_sat',
  y = 'error',
      color = 'real_earnings',
  tooltip=['predicted_earnings', 'real_earnings', 'tuition', 'adm_rate','avg_sat','per_student_spending','institution']
).properties(title="Average SAT score to prediction error")
chart1 | chart2 | chart3

"""After we had visualized the breakdown in errors and noticed some outliers in our graphs, we decided to see if we could train a binary logistic regression model to predict if the earnings would be below or above the average earnings. 

Our predictions would be binary results, and we were curious if this would result in more accuracy and "taming" of the outliers.  

First, we calculated the average earnings.

%%bigquery --project $project_id
SELECT AVG(cast(MN_EARN_WNE_P10 as INT64))
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
"""

#Training a 4-feature binary logistic regression model 
#instead of a linear regression model. Used same features as last model.
# %%bigquery --project $project_id

CREATE OR REPLACE MODEL `CollegeScorecard.earnings_model`
OPTIONS(model_type='logistic_reg') AS

SELECT 
  IF(cast(MN_EARN_WNE_P10 as INT64)<37454, 0, 1) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND ADM_RATE is not NULL 
  AND SAT_AVG is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=0

#Evaluating binary logistic regression model
# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  IF(cast(MN_EARN_WNE_P10 as INT64)<37454, 0, 1) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4

FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND ADM_RATE is not NULL 
  AND SAT_AVG is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=1))

#Testing binary logistic regression model

# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  IF(cast(MN_EARN_WNE_P10 as INT64)<37454, 0, 1) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND SAT_AVG is not NULL 
  AND ADM_RATE is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=2))

#Comparing test set predictions to "real" values
# %%bigquery --project $project_id to_graph_bin


SELECT
  INSTNM as institution, 
  IF(predicted_label=IF(real_earnings<37454, 0, 1),'Correct','Incorrect') as accuracy,
  predicted_label, 
  IF(real_earnings<37454, 0, 1) as real_label, real_earnings,
  feature1 as tuition, feature2 as adm_rate, feature3 as avg_sat, feature4 as per_student_spending
FROM
  ML.PREDICT(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  IF(cast(MN_EARN_WNE_P10 as INT64)<37454, 0, 1) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4,
  cast(MN_EARN_WNE_P10 as INT64) as real_earnings,
  INSTNM
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND SAT_AVG is not NULL 
  AND ADM_RATE is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=2))
ORDER BY accuracy desc

import altair as alt

chart1 = alt.Chart(to_graph_bin).mark_circle(size=50).encode(
  x = 'tuition',
  y = 'accuracy',
  color = 'real_earnings',
  tooltip=['real_earnings', 'tuition', 'adm_rate','avg_sat','per_student_spending','institution']
).properties(title="Tuition to prediction error")


chart2 = alt.Chart(to_graph_bin).mark_circle(size=50).encode(
  x = 'adm_rate',
  y = 'accuracy',
  color = 'real_earnings',
  tooltip=['real_earnings', 'tuition', 'adm_rate','avg_sat','per_student_spending','institution']
).properties(title="Admissions rate to prediction error")


chart3 = alt.Chart(to_graph_bin).mark_circle(size=50).encode(
  x = 'avg_sat',
  y = 'accuracy',
    color = 'real_earnings',
  tooltip=[ 'real_earnings', 'tuition', 'adm_rate','avg_sat','per_student_spending','institution']
).properties(title="Average SAT score to prediction error")




chart1|chart2|chart3

"""Just as we had done with our linear regression model, we were curious about the breakdown of "error" (in this case, "accuracy"). When we visualized error vs. tuition, admissions rate, and average SAT, we found that the inaccuracies no longer came from the outliers. Most of the incorrect predictions occured in the center of the distributions. Just as we had intended, we had reined in the outliers by simplifying our model to a binary regression. However, inaccuracies were still present."""

#Observing breakdown of binary results. 
# %%bigquery --project $project_id to_graph_bin

SELECT COUNT(CASE WHEN (predicted_label =0 AND (real_earnings<37454)) or (predicted_label =1 AND (real_earnings>=37454)) then 1 END) as total_correct,
COUNT(CASE WHEN (predicted_label =1 AND (real_earnings<37454)) or (predicted_label =0 AND (real_earnings>=37454)) then 1 END) as total_incorrect,
COUNT(CASE WHEN predicted_label =0 AND (real_earnings>=37454) then 1 END) as incorrect_underguessed,
COUNT(CASE WHEN predicted_label =1 AND (real_earnings<37454) then 1 END) as incorrect_overguessed

FROM
  ML.PREDICT(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  IF(cast(MN_EARN_WNE_P10 as INT64)<37454, 0, 1) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4,
  cast(MN_EARN_WNE_P10 as INT64) as real_earnings,
  INSTNM
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND SAT_AVG is not NULL 
  AND ADM_RATE is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=2))

"""Our model is correct most of the time. The accuracy isn't perfect, though. The accuracy is 87%. When it predicts incorrectly, the majority of the time our model "overguesses". In fact, 82% of the incorrect predictions are when our model predicts the earnings are higher than the average, when in reality they are not. If we had more time, we could pursue other factors that seem to correlate with lower-than-average earnings, and then incorporate those into our model. Hopefully, that would better identify schools where the earnings are below average by decreasing the amount of overguessing by the model.

Lastly, since the college scorecard data is so immense and our exploration barely skimmed the surface, we thought we would capitalize on all that data. To wrap up the project, we thought it'd be interesting and worthwhile to compare our "hand-made", 4-feature model to some more comprehensive models where we would pull data we hadn't explored yet from the College Scorecard Database. 

First, we added demographics features, including race and age.
"""

#Training model
# %%bigquery --project $project_id

CREATE OR REPLACE MODEL `CollegeScorecard.earnings_model`
OPTIONS(model_type='linear_reg') AS

SELECT 
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4,
  cast(UGDS_2MOR as float64) as feature5,
  cast(UGDS_WHITE as FLOAT64) as feature6,
  cast(UGDS_BLACK as FLOAT64) as feature7,
  cast(UGDS_HISP as FLOAT64) as feature8,
  cast(UGDS_ASIAN as FLOAT64) as feature9,
  cast(UGDS_AIAN as FLOAT64) as feature10,
  cast(UGDS_NHPI as FLOAT64) as feature11,
  cast(UG25ABV as FLOAT64) as feature12
  
  
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND MOD(int64_field_0,3)=0

#Evaluating model with added demographics data
# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
   cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4,
  cast(UGDS_2MOR as float64) as feature5,
  cast(UGDS_WHITE as FLOAT64) as feature6,
  cast(UGDS_BLACK as FLOAT64) as feature7,
  cast(UGDS_HISP as FLOAT64) as feature8,
  cast(UGDS_ASIAN as FLOAT64) as feature9,
  cast(UGDS_AIAN as FLOAT64) as feature10,
  cast(UGDS_NHPI as FLOAT64) as feature11,
  cast(UG25ABV as FLOAT64) as feature12
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND (NPT4_PUB is not NULL or NPT4_PRIV is not NULL)  
  AND ADM_RATE is not NULL 
  AND SAT_AVG is not NULL
  AND INEXPFTE is not NULL
  AND MOD(int64_field_0,3)=1))

"""Interestingly enough, adding those 7 demographics features worsened our model. So we pivoted, and added data about the financial backgrounds of students."""

#Training model
# %%bigquery --project $project_id

CREATE OR REPLACE MODEL `CollegeScorecard.earnings_model`
OPTIONS(model_type='linear_reg') AS

SELECT 
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4,
  cast(LO_INC_YR4_N as float64) as feature5,
  cast(MD_INC_YR4_N as float64) as feature6,
  cast(HI_INC_YR4_N as float64) as feature7,
  cast(FAMINC as float64) as feature8,
  cast(PCTPELL as FLOAT64) as feature9
  
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND LO_INC_YR4_N!='PrivacySuppressed'
  AND MD_INC_YR4_N!='PrivacySuppressed'
  AND HI_INC_YR4_N!='PrivacySuppressed'

  AND MOD(int64_field_0,3)=0

#Evaluating model with added financial demographics data
# %%bigquery --project $project_id

SELECT
  *
FROM
  ML.EVALUATE(MODEL `CollegeScorecard.earnings_model`, (
SELECT
  cast(MN_EARN_WNE_P10 as INT64) as label, 
  IF(NPT4_PUB is not NULL, NPT4_PUB, NPT4_PRIV) as feature1,
  cast(ADM_RATE as FLOAT64) as feature2,
  cast(SAT_AVG as FLOAT64) as feature3,
  cast(INEXPFTE as float64) as feature4,
  cast(LO_INC_YR4_N as float64) as feature5,
  cast(MD_INC_YR4_N as float64) as feature6,
  cast(HI_INC_YR4_N as float64) as feature7,
  cast(FAMINC as float64) as feature8,
  cast(PCTPELL as FLOAT64) as feature9
  
FROM `imposing-coast-218404.CollegeScorecard.data13_14`
WHERE 
  MN_EARN_WNE_P10!='PrivacySuppressed'
  AND LO_INC_YR4_N!='PrivacySuppressed'
  AND MD_INC_YR4_N!='PrivacySuppressed'
  AND HI_INC_YR4_N!='PrivacySuppressed'

  AND MOD(int64_field_0,3)=1))

"""Again, we found that adding more data actually made out linear regression model worse. 

This brief exploration of incorporating more data to our model was quite intruiging. Our initial model, with our 4 handpicked variables, was actually the most accurate.

%%bigquery --project $project_id summary

SELECT description as type, numfeatures as feature_count, error as mean_absolute_error, CONCAT(description,", ",(cast(numfeatures as STRING))," features ") as description
FROM `CollegeScorecard.errors`
ORDER BY error desc
"""

import altair as alt

alt.Chart(summary).mark_bar().encode(
  x = 'description',
  y = 'mean_absolute_error',
  color = 'feature_count',
  tooltip= ['type','feature_count','mean_absolute_error']
).properties(width=500,title="Summary of model performance by mean absolute error")

"""## Conclusions

### with Regards to the Data

We began by exploring the relationship between attributes that were related to the schools selectivity because intuitively the students at more selective institutions are already slated to expect higher earnings. We paired these features with the investment by the school (faculty salaries, for example) in student outcomes. These two categories, we realize, are far from independent; therefore some of the correlation we see in our plots for each may be confounded by the other. However, it is not surprising that for all of these features we found moderate correlations with earnings (admissions, SAT scores, $ of tuition, and spending per student). Seeing as we would be implementing a linear model, and since correlation coefficient is a linear metric, we felt that evaluating it would be a reasonable reference for the attribute's utility in the feature vector of our model inputs.

There are many other factors beyond those aforementioned that, of course, could contribute significantly to predicting earnings. Specifically, we would have liked to evaluate on a student level, based on demographic, and area of study, for example. While demographic information and area of study information for the school was included, it was included only as averages, preventing us from linking individuals to specific outcomes. In certain cases, we explored the relationship anyway--for example, looking at debt and financial aid (Pell grant). In both cases, we cannot say that it is the student WITH debt that end up having higher salaries --maybe their peers were totally debt-free and just making millions--nor could we say that it was the recipient of the Pell Grants that made less money (we saw a negative correlation between schools with high percentages of Pell recipients and future earnings). It is possible that the Pell Grant recipients did extremely well, but could only attend less selective institutions, which produce lower earnings, since they were unable to acrue the resume required for admittance to an Ivy. With regards to increased debt and increased earnings, which surprised us, maybe this could be explained by the fact that "fancier" schools are more expensive, which would confound the prior conclusion.

Our dataset is powerful in capturing a huge array of datapoints, but it also highlights how difficult it can be to parse out independent factors and causality in a complex world. With more time, we would have liked to have pursued that finer-grained student-level data, as not only would it inform about individuals, it would also inform about the role of institutions by separating the differences between variation. This would better shed light on the intricacies of accessibility and priveledge in education and incomes.

With regards to our models and our data explorations, correlations and accuracy were much better at the lower-income thresholds. Through discussion, we concluded that there are two major drivers in why earnings in high income thresholds are harder to predict: first, there are simply fewer people earning high incomes, and thus less data to train on (this applies only for the machine-learning model inaccuracy, not for the correlation discrepancy); second, and this is reflected in the graphs produced in our data exploration section, there are much greater *variability* in earnings in higher income brackets. This is to say that not only does pay increase, but so too does payscale, and as a consequence the top ten percent of earners vary more from each other than do the bottom 10. 

What have you learned? What conclusions have you made or been unable to make about your dataset and why? What is obvious, and what did you not expect to see? Support your statements with the charts or predictions you generated. If you had more time, what other data exploration would you pursue?


### with Regard to our Methods

An obvious limitation is that we are constrained to linear modeling. In addition to more data gathering described in the previous section, we could, with more time, and knowledge, try to overcome this limitation through feature engineering -- adding functions of existing features as new input features. We would need more understanding of the process in order to make good use of such freedom, however.  

We observed in comparing a model built with feature selection to one without that the former had better accuracy. In some sense, it is interesting that the training process wasn't able to compensate for the "extraneous" input features by simply lowering their theta weights to near zero-- in essence doing the feature selection for us-- but it also speaks to the fact that common sense and critical thinking, not data alone, are required for good modeling!

We also saw that our model predicted a greater number of overpredictions than underpredictions, which suggests that when it does underpredict, it underpredicts by a greater margin. Having seen our data from various angles, we believe that this is due to the fact that the high earners's incomes exponentially rise above the rest -- there is a bigger gap at the top. We were able to improve accuracy by using a logistic model, but at a high price of precision -- we could only say above or below average. This trade off is not unique to us: with any model, the stronger the claim, or more precise the prediction, the less accurate you evaluate to be because you are aiming for a smaller target. Nonetheless, it was a good first-hand reminder of the nature of such trade-offs.
"""